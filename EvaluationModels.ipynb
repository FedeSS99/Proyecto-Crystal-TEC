{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results from regression models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing results from the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from numpy import around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS2</th>\n",
       "      <th>MA</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.5723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.5373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.6395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MS2      MA      R2\n",
       "Linear         0.0460  0.0347  0.5723\n",
       "Decision Tree  0.0479  0.0345  0.5373\n",
       "Random Forest  0.0428  0.0317  0.6395"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scores for linear regrresion\n",
    "%store -r MS2_E_LR\n",
    "%store -r MA_E_LR\n",
    "%store -r R2_LR\n",
    "\n",
    "# Scores for Decision Tree regression\n",
    "%store -r MS2_E_DT\n",
    "%store -r MA_E_DT\n",
    "%store -r R2_DT\n",
    "\n",
    "# Scores for Random Forest regression\n",
    "%store -r MS2_E_RF\n",
    "%store -r MA_E_RF\n",
    "%store -r R2_RF\n",
    "\n",
    "Scores_Models = dict(MS2=[MS2_E_LR, MS2_E_DT, MS2_E_RF],\n",
    "                     MA=[MA_E_LR, MA_E_DT, MA_E_RF],\n",
    "                     R2=[R2_LR, R2_DT, R2_RF])\n",
    "\n",
    "Scores_Models = around(DataFrame(Scores_Models, index=[\"Linear\", \"Decision Tree\", \"Random Forest\"]), 4)\n",
    "Scores_Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first relevant observation for the three models is the \"intermedium\" values that we got with each one since the minimum and maximum $R^{2}$ are 0.537 and 0.639, respectively. The proposed cause for this to happen is that we have intrinsic bias in the data which is really hard to identify visually and even numerically (The outlier dropping with Mahalanobis distance shows this) and the same reason can be justified by considering the cultural context of the time and places in which the data was gather.\n",
    "\n",
    "Also the same data doesn't have a natural origin as a scientific experiment or even a technological device which one would expect to be \"precise\" but comes from artificial paramaters to study human populations by other humans."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the three models can be described as followed:\n",
    "- The linear model has a very good prediction of the violent crimes per population besides the simplicity of it with a $R^{2}$ of 0.572\n",
    "- The Decision Tree model falls in the category of worst predictor with a $R^{2}$ of 0.537, the reason it performs worse than the linear model could be associated with the way decision trees work\n",
    "- The Random Forest, even thouh it relies on decision trees, has the best prediction with a $R^{2}$ of 0.639 which highlights the strenght of this model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
