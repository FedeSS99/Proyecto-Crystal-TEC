{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results from regression models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing results from the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from numpy import around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS2</th>\n",
       "      <th>MA</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear</th>\n",
       "      <td>0.045</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.051</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MS2     MA     R2\n",
       "Linear         0.045  0.034  0.591\n",
       "Decision Tree  0.051  0.036  0.484\n",
       "Random Forest  0.042  0.031  0.655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scores for linear regrresion\n",
    "%store -r MS2_E_LR\n",
    "%store -r MA_E_LR\n",
    "%store -r R2_LR\n",
    "\n",
    "# Scores for Decision Tree regression\n",
    "%store -r MS2_E_DT\n",
    "%store -r MA_E_DT\n",
    "%store -r R2_DT\n",
    "\n",
    "# Scores for Random Forest regression\n",
    "%store -r MS2_E_RF\n",
    "%store -r MA_E_RF\n",
    "%store -r R2_RF\n",
    "\n",
    "Scores_Models = dict(MS2=[MS2_E_LR, MS2_E_DT, MS2_E_RF],\n",
    "                     MA=[MA_E_LR, MA_E_DT, MA_E_RF],\n",
    "                     R2=[R2_LR, R2_DT, R2_RF])\n",
    "\n",
    "Scores_Models = around(DataFrame(Scores_Models, index=[\"Linear\", \"Decision Tree\", \"Random Forest\"]), 3)\n",
    "Scores_Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first relevant observation for the three models is the \"intermedium\" values that we got with each one since the minimum and maximum $R^{2}$ are 0.484 and 0.655, respectively. The proposed cause for this to happen is that we have intrinsic bias in the data which is really hard to identify visually and even numerically (The outlier dropping with Mahalanobis distance shows this) and the same reason can be justified by considering the cultural context of the time and places in which the data was gather.\n",
    "\n",
    "Also the same data doesn't have a natural origin as a scientific experiment or even a technological device which one would expect to be \"precise\" but comes from artificial paramaters to study human populations by other humans."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the three models can be described as followed:\n",
    "- The linear model has a very good prediction of the violent crimes per population besides the simplicity of it with a $R^{2}$ of 0.591\n",
    "- The Decision Tree model falls in the category of worst predictor with a $R^{2}$ of 0.484, the reason of this could rely on the algorithm of this same model\n",
    "- The Random Forest, even thouh it relies on decision trees, has the best prediction with a $R^{2}$ of 0.655 which highlights the strenght of this model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
